---
title: "P8106 HW3"
author: "Brian Jo Hsuan Lee"
date: "3/22/2022"
output: pdf_document
---

Load packages

```{r, message=F}
library(tidyverse)
library(knitr)
library(AppliedPredictiveModeling)
library(pROC)
library(caret)
library(klaR)
library(MASS)
```

Import and tidy data

```{r, message=F}
data = read_csv("auto.csv") %>% 
  mutate(
    year = factor(year),
    origin = factor(origin),
    mpg_cat = factor(mpg_cat),
    mpg_cat = fct_relevel(mpg_cat, c("low", "high"))
  )
```

Partition the data for model training

```{r}
set.seed(2022)

# partition data into training and testing sets as randomized 7:3 splits
train_index = createDataPartition(y = data$mpg_cat, p = 0.7, list = FALSE)
train_data = data[train_index, ]
train_cont_data = data[train_index, -6:-7]
test_data = data[-train_index, ]

# matrices of predictors 
train_pred = model.matrix(mpg_cat ~ ., train_data)[ ,-1]
train_cont_pred = model.matrix(mpg_cat ~ ., train_data)[ , 2:6]
# test_pred = model.matrix(mpg_cat ~ ., test_data)[ ,-1]
# test_cont_pred = model.matrix(mpg_cat ~ ., train_data)[ , 2:6]

# vectors of response
train_resp = train_data$mpg_cat
test_resp = test_data$mpg_cat
```

## a)
**Produce data summaries**

Calculate descriptive statistics for the training data: quantile data for the continuous variables and count data for the categorical variables. Number of cylinders is arguably an ordinal categorical variable but is treated as a continuous variable here. Most cars have an American origin (category 1), and the number of high and low mileage car samples are the same.

```{r}
summary(train_data)
```

Visualize training data distribution. In general, cars with high mileage have lower weights, cylinder count, engine displacement in inches, and horsepower. Note the unequal distribution of car count when conditioned on their origin and mileage. 

```{r}
# set graphic theme
trellis.par.set(transparentTheme(trans = .4))

# plot distribution of the continuous predictors
featurePlot(train_cont_pred, train_resp,
            scales = list(x = list(relation = "free"), 
                          y = list(relation = "free")),
            plot = "density", pch = "|", 
            auto.key = list(columns = 2))

# plot distribution of the categorical predictors
train_data %>% 
  count(year, origin, mpg_cat) %>% 
  ggplot(aes(x = year, y = n, fill = origin)) + 
  geom_col() +
  facet_grid(cols = vars(origin), rows = vars(mpg_cat)) +
  labs(
    title = "Car Distribution Across Year by Origin and Mileage",
    x = "Year (19-)",
    y = "Count"
  ) +
  theme(
    plot.title = element_text(hjust = 0.5),
    legend.position = "bottom"
  )
```

## b)
**Logit Regression**

Fit a logit model and list its significant coefficients, which are car weight, model year 79, model year 81, and European origin.
```{r}
logit_fit = glm(mpg_cat ~ ., 
            data = train_data, 
            family = binomial(link = "logit"))
summary(logit_fit)
```

Build a confusion matrix, and extract the overall fraction of correct predictions. This confusion matrix demonstrates the number of correct predictions generated by our logit model. The rows correspond to what the model predicted, and the columns correspond to the known truths; the number of true lows, true highs, false lows, and false highs are 49, 53, 5, and 9, respectively. The overall fraction of correct predictions could be calculated by $\frac{49+53}{49+9+5+53} = 0.880$.
```{r}
# use the model to forecast new observations provided by the testing data
logit_pred_prob = predict(logit_fit, newdata = test_data,
                          type = "response")

# create a vector that is equal in length to the testing data and holds the predicted binary result
logit_pred = rep("low", length(logit_pred_prob))
logit_pred[logit_pred_prob > 0.5] = "high"

# create a confusion matrix
logit_cm = confusionMatrix(data = factor(logit_pred, levels = c("low", "high")),
                                         reference = test_resp,
                                         positive = "high")

# display the matrix
kable(logit_cm$table, "simple")

# extract overall correctness of the model
logit_cm$byClass["Balanced Accuracy"]
```

## c)
**Multivariate Adaptive Regression Spline**

Set the resampling method for model fitting functions in the caret package.
```{r}
ctrl = trainControl(method = "repeatedcv", repeats = 5, number = 10,
                    summaryFunction = twoClassSummary,
                    classProbs = TRUE)
```

Fit a MARS model.
```{r, warning=F, message=F}
set.seed(2022)

# enable a grid of the 2 potential tuning parameters: degree of interactions and the number of retained terms
mars_grid = expand.grid(degree = 1:3, nprune = 2:20)

# fit and show
mars_fit = train(x = train_pred,
                 y = train_resp,
                 method = "earth",
                 tuneGrid = mars_grid,
                 metric = "ROC",
                 trControl = ctrl)
plot(mars_fit)

# extract the optimal tuning parameters
kable(mars_fit$bestTune, "simple")
```

## d)
**LDA/QDA**

Bonus: Create a partition plot for LDA exploratory data analysis. LDA and QDA do not handle categortical predictor well and they best be excluded. 
```{r}
partimat(mpg_cat ~ ., 
         method = "lda", 
         data = train_cont_data, 
         nplots.vert = 2)
```

Fit a LDA model.
```{r}
lda_fit = lda(mpg_cat ~ ., data = train_cont_data)

# display linear discriminants
plot(lda_fit)
```

Fit a QDA model.
```{r}
set.seed(2022)

qda_fit = train(x = train_cont_pred,
                y = train_resp,
                method = "qda",
                metric = "ROC",
                trControl = ctrl)
```

## e) 
**Model Selection and justification**

Compare ROC. Resamples() compares caret models, so we need to recreate a logit and an LDA model using the caret package 
```{r, message=F}
set.seed(2022)

logit_fit_caret = train(x = train_pred,
                        y = train_resp,
                        method = "glm",
                        metric = "ROC",
                        trControl = ctrl)

lda_fit_caret = train(x = train_cont_pred,
                      y = train_resp,
                      method = "lda",
                      metric = "ROC",
                      trControl = ctrl)

train_res = resamples(list(Logit = logit_fit_caret, 
                           MARS = mars_fit,
                           LDA = lda_fit_caret, 
                           QDA = qda_fit))
summary(train_res)

bwplot(train_res, metric = "ROC")
```

```{r, message=F}
logit_roc = roc(test_resp, logit_pred_prob)
plot(logit_roc, legacy.axes = TRUE, print.auc = TRUE) 
plot(smooth(logit_roc), col = 4, add = TRUE)
```

