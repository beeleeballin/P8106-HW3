---
title: "P8106 HW3"
author: "Brian Jo Hsuan Lee"
date: "3/22/2022"
output: pdf_document
---

Load packages

```{r, message=F}
library(tidyverse)
library(AppliedPredictiveModeling)
library(pROC)
library(caret)
library(klaR)
library(MASS)
```

Import and tidy data

```{r, message=F}
data = read_csv("auto.csv") %>% 
  mutate(
    year = factor(year),
    origin = factor(origin),
    mpg_cat = factor(mpg_cat),
    mpg_cat = fct_relevel(mpg_cat, c("low", "high"))
  )
```

Partition the data for model training

```{r}
set.seed(2022)

# partition data into training and testing sets into randomized 4:1 splits
train_index = createDataPartition(y = data$mpg_cat, p = 0.7, list = FALSE)
train_data = data[train_index, ]
test_data = data[-train_index, ]

# matrices of predictors 
train_pred = model.matrix(mpg_cat ~ ., train_data)[ ,-1]
train_cont_pred = model.matrix(mpg_cat ~ ., train_data)[ , 2:6]
test_pred = model.matrix(mpg_cat ~ ., test_data)[ ,-1]
test_cont_pred = model.matrix(mpg_cat ~ ., train_data)[ , 2:6]

# vectors of response
train_resp = train_data$mpg_cat
test_resp = test_data$mpg_cat
```

Calculate descriptive statistics: quantile data for the continuous variables and count data for the categorical variables.
Number of cylinders is arguably an ordinal categorical variable but is treated as a continuous variable here. Most cars have an American origin (category 1), and the number of high and low mileage car samples are the same.

```{r}
summary(train_data)
```

Visualize data distribution. In general, cars with high mileage have lower weights, cylinder count, engine displacement in inches, and horsepower. Note the unequal distribution of car count when conditioned on their origin and mileage. 

```{r}
trellis.par.set(transparentTheme(trans = .4))

featurePlot(train_pred[, 1:5], train_resp,
            scales = list(x = list(relation = "free"), 
                          y = list(relation = "free")),
            plot = "density", pch = "|", 
            auto.key = list(columns = 2))

train_data %>% 
  count(year, origin, mpg_cat) %>% 
  ggplot(aes(x = year, y = n, fill = origin)) + 
  geom_col() +
  facet_grid(cols = vars(origin), rows = vars(mpg_cat)) +
  labs(
    title = "Car Distribution Across Year by Origin and Mileage",
    x = "Year (19-)",
    y = "Count"
  ) +
  theme(
    plot.title = element_text(hjust = 0.5),
    legend.position = "bottom"
  )
```

```{r}
logit_fit = glm(mpg_cat ~ ., 
            data = train_data, 
            family = binomial(link = "logit"))
summary(logit_fit)
```

```{r}
logit_pred_prob = predict(logit_fit, newdata = test_data,
                          type = "response")
logit_pred = rep("low", length(logit_pred_prob))
logit_pred[logit_pred_prob > 0.5] = "high"

confusionMatrix(data = factor(logit_pred, levels = c("low", "high")),
                reference = test_resp,
                positive = "high")
```

```{r}
logit_roc = roc(test_resp, logit_pred_prob)
plot(logit_roc, legacy.axes = TRUE, print.auc = TRUE) 
plot(smooth(logit_roc), col = 4, add = TRUE)
```

```{r}
ctrl = trainControl(method = "repeatedcv", repeats = 5, number = 10,
                    summaryFunction = twoClassSummary,
                    classProbs = TRUE)
```

```{r}
set.seed(2022)

mars_grid = expand.grid(degree = 1:3, nprune = 2:20)
mars_fit = train(x = train_pred,
                 y = train_resp,
                 method = "earth",
                 tuneGrid = mars_grid,
                 metric = "ROC",
                 trControl = ctrl)

plot(mars_fit)

mars_fit$bestTune

coef(mars_fit$finalModel) 
```

```{r}
partimat(mpg_cat ~ cylinders + displacement + horsepower + weight + acceleration, 
          method = "lda", data = train_data, nplots.vert = 2)
```

```{r}
lda_fit = lda(mpg_cat ~ ., data = train_data[, -6:-7])
plot(lda_fit)
```

```{r}
set.seed(2022)

qda_fit = train(x = train_cont_pred,
                y = train_resp,
                method = "qda",
                metric = "ROC",
                trControl = ctrl)
```

Compare ROC. Resamples() compares caret models, so we need to recreate a logit and an LDA model using the caret package 
```{r}
set.seed(2022)

logit_fit_caret = train(x = train_pred,
                        y = train_resp,
                        method = "glm",
                        metric = "ROC",
                        trControl = ctrl)

lda_fit_caret = train(x = train_cont_pred,
                      y = train_resp,
                      method = "lda",
                      metric = "ROC",
                      trControl = ctrl)

train_res = resamples(list(Logit = logit_fit_caret, 
                           MARS = mars_fit,
                           LDA = lda_fit_caret, 
                           QDA = qda_fit))
summary(train_res)

bwplot(train_res, metric = "ROC")
```

